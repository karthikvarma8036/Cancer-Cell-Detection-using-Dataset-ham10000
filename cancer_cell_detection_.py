# -*- coding: utf-8 -*-
"""Cancer cell detection .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fmzEWiC7XJ_0yyitUtlUHlyj-mz46-lZ
"""

!pip install opendatasets

pip install pandas

import opendatasets as od
import pandas
od.download(
	"https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000")

import numpy as np
import tensorflow as tf
import PIL
import os
import random
import pandas as pd
import matplotlib.pyplot as plt
import gc # garbage collector

def plot_Model_lossAcc(his):
    plt.figure(figsize = (16,8))

    plt.subplot(2,2,1)
    plt.plot(his.history['loss'])
    plt.plot(his.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Training', 'Validation'], loc='upper right')
    plt.show()

    plt.figure(figsize = (16,8))

    plt.subplot(2,2,2)
    plt.plot(his.history['accuracy'])
    plt.plot(his.history['val_accuracy'])
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Training', 'Validation'], loc='lower right')
    plt.show()

path = '/content/skin-cancer-mnist-ham10000/'
p1 = 'HAM10000_images_part_1/'
p2 = 'HAM10000_images_part_2/'
meta = 'HAM10000_metadata.csv'

cancer_df = pd.read_csv(path+meta)

print(cancer_df['dx'].unique())
classnames = cancer_df['dx'].unique()
separate_dict = {}

for unique_class in classnames:
    temp = cancer_df.loc[cancer_df['dx'] == unique_class,['image_id']]
    separate_dict[unique_class] = temp['image_id'].tolist()
print(separate_dict.keys())

name = [(os.listdir(path + x)) for x in os.listdir(path) if os.path.isdir(path + x)]
name.remove(name[3]) # Removes duplicate entries
name.remove(name[1]) # Removes duplicate entries
all_names = name[0]+name[1] # Creates a list of all image names
all_path = [path+p1+x for x in name[0]]+[path+p2+x for x in name[1]] # Creates a list of all image paths

# Generate one hot encodings for all classes
encoder_keys = list(separate_dict.keys())
encoder_template = {}
for i in encoder_keys:
    if i == 'bkl':
        encoder_template[i] = [1,0,0,0,0,0,0]
    elif i == 'nv':
        encoder_template[i] = [0,1,0,0,0,0,0]
    elif i == 'df':
        encoder_template[i] = [0,0,1,0,0,0,0]
    elif i == 'mel':
        encoder_template[i] = [0,0,0,1,0,0,0]
    elif i == 'vasc':
        encoder_template[i] = [0,0,0,0,1,0,0]
    elif i == 'bcc':
        encoder_template[i] = [0,0,0,0,0,1,0]
    elif i == 'akiec':
        encoder_template[i] = [0,0,0,0,0,0,1]

print(encoder_template)

# Generate one hot encodings
all_encodings = []
for img_name in all_names:
    temp1 = img_name.split('.')
    if temp1[0] in separate_dict['bkl']:
        all_encodings.append(encoder_template['bkl'])

    elif temp1[0] in separate_dict['nv']:
        all_encodings.append(encoder_template['nv'])

    elif temp1[0] in separate_dict['df']:
        all_encodings.append(encoder_template['df'])

    elif temp1[0] in separate_dict['mel']:
        all_encodings.append(encoder_template['mel'])

    elif temp1[0] in separate_dict['vasc']:
        all_encodings.append(encoder_template['vasc'])

    elif temp1[0] in separate_dict['bcc']:
        all_encodings.append(encoder_template['bcc'])

    elif temp1[0] in separate_dict['akiec']:
        all_encodings.append(encoder_template['akiec'])

print(len(all_encodings))

# Assign one hot encodings to all the image paths in sync
all_path_encodings = list(zip(all_path,all_encodings))
random.shuffle(all_path_encodings)
all_path,all_encodings = zip(*all_path_encodings)

print(all_path_encodings[0])

data_pipeline = tf.data.Dataset.from_tensor_slices((list(all_path),list(all_encodings)))

del all_path, all_encodings, all_path_encodings, all_names, name, separate_dict, encoder_keys, encoder_template, classnames, cancer_df
gc.collect()

def load_images_and_process(image_path,one_hot_vec):
    '''load images and decode jpeg'''
    image = tf.io.read_file(image_path)
    image = tf.image.decode_jpeg(image, channels=3)
#     return image,one_hot_vec

# def process_image(image,one_hot_vec):
    '''normalise from 0-255 to 0-1'''
#     image = tf.cast(image, tf.float16) / 255.0
    image = tf.image.resize(image, [90,120])
    image = image/255
    return image,one_hot_vec

data_pipeline = data_pipeline.map(load_images_and_process).prefetch(tf.data.AUTOTUNE)#.map(process_image)

# Train, test, validation split
train_size = int(0.8*len(data_pipeline))
val_size = int(0.1*len(data_pipeline))
test_size =int(len(data_pipeline)) - train_size - val_size
print(len(data_pipeline),train_size,val_size,test_size)

train_ds = data_pipeline.take(train_size)
val_ds = data_pipeline.take(val_size)
test_ds = data_pipeline.take(test_size)


train_ds = train_ds.shuffle(10000)
val_ds = val_ds.shuffle(1000)
test_ds = test_ds.shuffle(1000)

print(len(val_ds),len(train_ds),len(test_ds))
del data_pipeline
gc.collect()

# Model building

model = tf.keras.Sequential()

model.add(tf.keras.layers.Conv2D(30, (5,5),strides = (1,1), padding = 'valid', activation = 'relu', input_shape = (90,120, 3)))
# model.add(tf.keras.layers.Conv2D(32, (3,3),strides = (1,1), padding = 'valid', activation = 'relu', input_shape = (32,32,2)))
model.add(tf.keras.layers.Conv2D(30,(3,3),strides = (1,1), padding = 'valid', activation = 'relu'))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides = None, padding = 'valid'))

model.add(tf.keras.layers.Conv2D(20,(3,3), strides = (1,1), padding = 'valid', activation = 'relu'))
model.add(tf.keras.layers.Conv2D(15,(3,3),strides = (1,1), padding = 'valid', activation = 'relu'))
# model.add(tf.keras.layers.GroupNormalization(groups=8))
# model.add(tf.keras.layers.BatchNormalization())

model.add(tf.keras.layers.Conv2D(15, (3,3),strides = (1,1), padding = 'valid', activation = 'relu'))
model.add(tf.keras.layers.GroupNormalization(groups=3))
# model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides = None, padding = 'valid'))

# model.add(tf.keras.layers.Conv2D(8, (3,3),strides = (1,1), padding = 'valid', activation = 'relu'))
# model.add(tf.keras.layers.Conv2D(8, (3,3),strides = (1,1), padding = 'valid', activation = 'relu'))
# model.add(tf.keras.layers.GroupNormalization(groups=8))
# # model.add(tf.keras.layers.Conv2D(6, (3,3),strides = (1,1), padding = 'valid', activation = 'relu'))
# model.add(tf.keras.layers.GroupNormalization(groups=8))

# model.add(tf.keras.layers.Conv2D(6, (3,3),strides = (1,1), padding = 'valid', activation = 'relu'))
# model.add(tf.keras.layers.GroupNormalization(groups=6))

model.add(tf.keras.layers.Conv2D(10, (3,3),strides = (1,1), padding = 'valid', activation = 'relu'))
# model.add(tf.keras.layers.GroupNormalization(groups=6))

# model.add(tf.keras.layers.Conv2D(64, (3,3),strides = (1,1), padding = 'valid', activation = 'relu'))
# model.add(tf.keras.layers.Conv2D(64, (3,3),strides = (1,1), padding = 'valid', activation = 'relu'))

# model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides = None, padding = 'valid'))
# model.add(tf.keras.layers.Conv2D(8,(3,3), strides = (1,1), padding = 'valid', activation = 'relu'))
# model.add(tf.keras.layers.Conv2D(4,(3,3), strides = (1,1), padding = 'valid', activation = 'relu'))


model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Normalization())
# model.add(tf.keras.layers.Dropout(0.3))
# model.add(tf.keras.layers.Dense(1024,activation = 'relu'))
model.add(tf.keras.layers.Dense(256,activation = 'relu'))

# model.add(tf.keras.layers.LayerNormalization())
# model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dropout(0.1))
model.add(tf.keras.layers.Dense(128,activation = 'relu'))
# model.add(tf.keras.layers.Normalization())

# model.add(tf.keras.layers.Dense(64,activation = 'relu'))
# model.add(tf.keras.layers.Normalization())

# model.add(tf.keras.layers.Dense(128,activation = 'relu'))

model.add(tf.keras.layers.Dense(7,activation = 'softmax'))

model.compile(optimizer = tf.keras.optimizers.Adam(), loss = 'categorical_crossentropy', metrics = ['accuracy'])

model.summary()

epochs = 20
batch_size = 8

checkpoint_filepath = '/content/'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=False, monitor='val_accuracy')
EarlyStop = tf.keras.callbacks.EarlyStopping(monitor='EarlyStopping', patience=5)
def scheduler(epoch, lr):
    flr = 1e-3
    if epoch < epochs*0.1 :
        return flr
    elif epoch > epochs*0.1 and epoch < epochs*0.25:
        lr *= tf.math.exp(-0.1)
        return lr
    else:
        lr *= tf.math.exp(-0.008)
#         return lr * tf.math.exp(-0.004)
        return lr
learning_rate_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)

# Memory clear on every epoch
class MemoryClearCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        gc.collect()
memory_clear_callback = MemoryClearCallback()

history = model.fit(train_ds.batch(batch_size),epochs=epochs,validation_data=val_ds.batch(batch_size),callbacks=[model_checkpoint_callback,EarlyStop,memory_clear_callback],shuffle=True)

plot_Model_lossAcc(history)

import shutil
shutil.make_archive('recentbest777', 'zip', '/content')